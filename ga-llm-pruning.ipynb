{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installations","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/cg123/mergekit.git && cd mergekit && git reset --hard c891a0900969c1fb8ce678e3ace0e084a92c24fc && cd ..","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:41:45.519004Z","iopub.execute_input":"2025-02-04T07:41:45.519328Z","iopub.status.idle":"2025-02-04T07:41:46.452286Z","shell.execute_reply.started":"2025-02-04T07:41:45.519299Z","shell.execute_reply":"2025-02-04T07:41:46.451545Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'mergekit'...\nremote: Enumerating objects: 2528, done.\u001b[K\nremote: Counting objects: 100% (981/981), done.\u001b[K\nremote: Compressing objects: 100% (269/269), done.\u001b[K\nremote: Total 2528 (delta 850), reused 721 (delta 712), pack-reused 1547 (from 3)\u001b[K\nReceiving objects: 100% (2528/2528), 853.60 KiB | 15.81 MiB/s, done.\nResolving deltas: 100% (1721/1721), done.\nHEAD is now at c891a09 Make Cohere lm_head optional (#417)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade pip\n%cd mergekit\n!pip install -e . -U --quiet --root-user-action=ignore\n%cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:41:46.453571Z","iopub.execute_input":"2025-02-04T07:41:46.453900Z","iopub.status.idle":"2025-02-04T07:42:04.818869Z","shell.execute_reply.started":"2025-02-04T07:41:46.453863Z","shell.execute_reply":"2025-02-04T07:42:04.817934Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\nDownloading pip-25.0-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.0\n/kaggle/working/mergekit\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building editable for mergekit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.12 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m/kaggle/working\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!git clone -b external_ds_download https://github.com/ahmedamrelhefnawy/lm-evaluation-harness\n%cd lm-evaluation-harness\n!pip install -q -e . -U\n%cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:04.820793Z","iopub.execute_input":"2025-02-04T07:42:04.821079Z","iopub.status.idle":"2025-02-04T07:42:25.443842Z","shell.execute_reply.started":"2025-02-04T07:42:04.821054Z","shell.execute_reply":"2025-02-04T07:42:25.442873Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'lm-evaluation-harness'...\nremote: Enumerating objects: 29360, done.\u001b[K\nremote: Total 29360 (delta 0), reused 0 (delta 0), pack-reused 29360 (from 1)\u001b[K\nReceiving objects: 100% (29360/29360), 23.18 MiB | 21.56 MiB/s, done.\nResolving deltas: 100% (19266/19266), done.\n/kaggle/working/lm-evaluation-harness\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n/kaggle/working\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Login","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\ntoken= \"**\" # Put your token here\nlogin(token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:25.445565Z","iopub.execute_input":"2025-02-04T07:42:25.445846Z","iopub.status.idle":"2025-02-04T07:42:26.230214Z","shell.execute_reply.started":"2025-02-04T07:42:25.445821Z","shell.execute_reply":"2025-02-04T07:42:26.229578Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"markdown","source":"Enable max download speed from huggingface","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:26.230913Z","iopub.execute_input":"2025-02-04T07:42:26.231108Z","iopub.status.idle":"2025-02-04T07:42:26.234706Z","shell.execute_reply.started":"2025-02-04T07:42:26.231091Z","shell.execute_reply":"2025-02-04T07:42:26.233835Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import gc\nimport os\n\nfrom transformers import AutoModel, AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline, logging\nimport datasets\n\nimport tensorflow as tf\nimport torch\nimport numpy as np\n\nimport yaml\nimport json\nimport copy\nimport random\nimport time\n\nfrom deap import base, tools, creator, algorithms\n\nimport matplotlib.pyplot as plt\n\n\n%cd ./mergekit\nfrom mergekit.config import MergeConfiguration\nfrom mergekit.merge import MergeOptions, run_merge\n%cd ..\n\n%cd ./lm-evaluation-harness\nfrom lm_eval import simple_evaluate, downloading_tasks\n%cd ..\n\n# Set the verbosity to ERROR to suppress warnings\nlogging.set_verbosity_error()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:26.235625Z","iopub.execute_input":"2025-02-04T07:42:26.235901Z","iopub.status.idle":"2025-02-04T07:42:47.834416Z","shell.execute_reply.started":"2025-02-04T07:42:26.235873Z","shell.execute_reply":"2025-02-04T07:42:47.833443Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/mergekit\n/kaggle/working\n/kaggle/working/lm-evaluation-harness\n/kaggle/working\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Custom Model Class","metadata":{"execution":{"iopub.status.busy":"2025-01-27T18:56:00.409709Z","iopub.execute_input":"2025-01-27T18:56:00.410075Z","iopub.status.idle":"2025-01-27T18:56:00.414388Z","shell.execute_reply.started":"2025-01-27T18:56:00.410048Z","shell.execute_reply":"2025-01-27T18:56:00.413202Z"}}},{"cell_type":"code","source":"class custom_model:\n    def __init__(self,\n                 base_model: str,\n                 dtype: str,\n                 num_layers: int,\n                 layers: list[int] = None,\n                 path: str = \"/content/merged\") -> None:\n\n        self.base_model = base_model\n        self.num_layers = num_layers\n        self.layers = layers if layers else [i for i in range(self.num_layers)]\n        self.dtype = dtype\n        self.path = path\n\n    @property\n    def config(self):\n\n        sources = []\n        for j in self.layers:\n            sources.append(                {\n                    'model': self.base_model,\n                    'layer_range': [j, j+1]\n                })\n\n        config_dict = {\n                        'slices': [\n                            {\n                                'sources': sources\n                            }\n                        ],\n                        'merge_method': 'passthrough',\n                        'dtype': self.dtype\n                    }\n        # Convert dictionary to YAML\n        config_text = yaml.dump(config_dict, sort_keys=False, default_flow_style=None) \n\n        return config_text\n\n    @config.setter\n    def config(self, text: str):\n        self.config = text\n\n    def remove_layers(self, removed_layers: list[int]):\n        for item in removed_layers:\n            self.layers.remove(item)\n    \n    def reset_layers(self):\n        self.layers = [i for i in range(self.num_layers)]\n        \n    def build(self,\n              config_yaml_path,\n              lora_merge_cache= \"/tmp\",\n              cuda= bool(tf.config.list_physical_devices('GPU')),\n              copy_tokenizer=True,\n              lazy_unpickle=False,\n              low_cpu_memory=False,\n              trust_remote_code=True,\n              allow_crimes=True):\n\n        # Write Down Config\n        with open(config_yaml_path, 'w') as cf:\n            cf.write(self.config)\n\n        # Actually do merge\n        with open(config_yaml_path, \"r\", encoding=\"utf-8\") as fp:\n            merge_config = MergeConfiguration.model_validate(\n                yaml.safe_load(fp))\n\n        run_merge(\n            merge_config,\n            out_path=self.path,\n            options=MergeOptions(\n\n                lora_merge_cache=lora_merge_cache,\n                cuda=cuda,\n                copy_tokenizer=copy_tokenizer,\n                lazy_unpickle=lazy_unpickle,\n                low_cpu_memory=low_cpu_memory,\n                trust_remote_code=trust_remote_code,\n                allow_crimes=allow_crimes\n            ),\n        )\n\n    def evaluate(self, func, *args):\n        return func(*args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:47.835534Z","iopub.execute_input":"2025-02-04T07:42:47.835868Z","iopub.status.idle":"2025-02-04T07:42:48.062642Z","shell.execute_reply.started":"2025-02-04T07:42:47.835835Z","shell.execute_reply":"2025-02-04T07:42:48.061449Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Genetic Algorithm","metadata":{}},{"cell_type":"markdown","source":"## Globals","metadata":{}},{"cell_type":"code","source":"# Merge Parameters\nOUTPUT_PATH = \"/content/merged\"  # folder to store the result in\nLORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\nCONFIG_YAML = \"/kaggle/working/config.yaml\"  # merge configuration file\nCOPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\nLAZY_UNPICKLE = False  # experimental low-memory model loader\nLOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:48.065161Z","iopub.execute_input":"2025-02-04T07:42:48.065412Z","iopub.status.idle":"2025-02-04T07:42:48.085922Z","shell.execute_reply.started":"2025-02-04T07:42:48.065391Z","shell.execute_reply":"2025-02-04T07:42:48.085179Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"BASE_MODEL_SCORES = {'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B': {'time': 1889.357815027237,\n                          'batch_size':2 },\n                    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:48.087113Z","iopub.execute_input":"2025-02-04T07:42:48.087353Z","iopub.status.idle":"2025-02-04T07:42:48.102001Z","shell.execute_reply.started":"2025-02-04T07:42:48.087332Z","shell.execute_reply":"2025-02-04T07:42:48.101363Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define random seed\nrandom.seed(25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:48.102813Z","iopub.execute_input":"2025-02-04T07:42:48.103038Z","iopub.status.idle":"2025-02-04T07:42:48.114759Z","shell.execute_reply.started":"2025-02-04T07:42:48.103009Z","shell.execute_reply":"2025-02-04T07:42:48.114011Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def clear_cache():\n    for var_name in list(globals().keys()):\n        var = globals()[var_name]\n        if isinstance(var, (AutoModel, AutoTokenizer)):\n            del globals()[var_name]\n\n    for var_name in list(locals().keys()):\n        var = locals()[var_name]\n        if isinstance(var, (AutoModel, AutoTokenizer)):\n            del locals()[var_name]\n    \n    tf.keras.backend.clear_session()\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:48.115429Z","iopub.execute_input":"2025-02-04T07:42:48.115677Z","iopub.status.idle":"2025-02-04T07:42:48.129531Z","shell.execute_reply.started":"2025-02-04T07:42:48.115659Z","shell.execute_reply":"2025-02-04T07:42:48.128715Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Algorithm","metadata":{}},{"cell_type":"code","source":"def run_ga(custom_model,\n           num_pruned_layers,\n           tasks,\n           metrics,\n           pop_size = 60,\n           max_gens = 10,\n           tourn_size= 5,\n           p_cx= 0.9,\n           p_mu= 0.5,\n           hall_of_fame = 1,\n           prev_results : dict = {}):\n    \n    custom_model.reset_layers()\n\n    toolbox = base.Toolbox()\n    \n    generated = set(prev_results)\n\n    def sortedRandomPick(num_layers, num_pruned_layers):        \n        chromosome = sorted(random.sample(range(2, num_layers), num_pruned_layers))\n    \n        while tuple(chromosome) in generated:\n            chromosome = sorted(random.sample(range(2, num_layers), num_pruned_layers))\n    \n        generated.add(tuple(chromosome))\n    \n        return chromosome\n\n    # Create the LayersPicker operator\n    toolbox.register('LayersPicker', sortedRandomPick, custom_model.num_layers, num_pruned_layers)\n    \n    # Create the fitness class\n    creator.create('FitnessMin', base.Fitness, weights=(1,)) # -1 => Minimum || 1 => Maximum\n    \n    # Create the Chromosome class\n    creator.create('Chromosome', list, fitness= creator.FitnessMin)\n    \n    # Register the ChromosomeCreator operator\n    toolbox.register('ChromosomeCreator', tools.initIterate, creator.Chromosome, toolbox.LayersPicker)\n    \n    # Register the PopulationCreator operator\n    toolbox.register('PopulationCreator', tools.initRepeat, list, toolbox.ChromosomeCreator)\n    \n    \n    # Define a function to evaluate the error of the selected model\n    evaluated_dict = prev_results\n    \n    def evaluate(chromosome):    \n        # Clear Cache if exists\n        try:\n            del hfmodel, hfpipe\n            clear_cache()\n        except:\n            pass\n    \n        # Remove chromosomes with insufficient number of genes\n        if len(set(chromosome)) != num_pruned_layers:\n            return (0,)\n    \n        # Chromosome key preprocessing\n        chromosome = sorted(chromosome)\n        chromosome_key = tuple(chromosome)\n    \n        # Evaluate the chromosome\n        if chromosome_key in evaluated_dict:\n            evaluated_dict[chromosome_key]['counter'] += 1\n    \n            score = evaluated_dict[chromosome_key]['score']\n    \n        else:\n    \n            # test_model \n            custom_model.reset_layers()\n    \n            custom_model.remove_layers(chromosome)\n    \n            custom_model.build(config_yaml_path= CONFIG_YAML,\n                            lora_merge_cache= \"/tmp\",\n                            # cuda= \n                            )\n\n            # Start Timer\n            start = time.time()\n\n            # Start task evaluation\n            results = simple_evaluate(\n                                    model=\"hf\",\n                                    model_args= {'pretrained': custom_model.path,\n                                                 'trust_remote_code': True},\n                                    tasks= ['hellaswag', 'mutual', 'piqa', 'truthfulqa_mc2', 'gsm8k', 'lambada_openai', 'lambada_standard', 'winogrande', 'openbookqa'],\n                                    log_samples= True,\n                                    device= \"cuda:0\",\n                                    batch_size = \"auto:4\",\n                                    limit = 100,\n                                    task_dict = tasks,\n                                    apply_chat_template= True,\n                                    )\n\n            # Stop Timer and save the result\n            model_time = time.time() - start\n            base_model_time = BASE_MODEL_SCORES[custom_model.base_model]\n            speed_score = (base_model_time**0.5) * (model_time**0.5)\n            \n            score, n_tasks = 0, 0\n            metrics = metrics\n            for task, metric in zip(results['results'], metrics):\n        \n                n_tasks += 1\n                task_results = results['results'][task]\n                score += task_results[metric] ** 2\n            \n            score = score / n_tasks\n                \n            evaluated_dict[chromosome_key] = {}\n            evaluated_dict[chromosome_key]['task_score'] = score\n            evaluated_dict[chromosome_key]['score'] = score\n            \n        print(\"-\"*30)\n        print(f'Removed: {chromosome} --> Score: {score} %')\n        print(\"-\"*30)\n        print(\" \")\n        \n        # Save in JSON File\n        evaluated_dict_str_keys = {str(k): v for k, v in evaluated_dict.items()}\n        with open('/kaggle/working/evaluated_dict.json', 'w') as json_file:\n            json.dump(evaluated_dict_str_keys, json_file, indent=4)\n        \n        return (score,)\n    # Register the evaluation function\n    toolbox.register('evaluate', evaluate)\n    \n    # Create the genetic operators\n    toolbox.register('select', tools.selTournament, tournsize= tourn_size)\n    \n    toolbox.register('mate', tools.cxOnePoint)\n    \n    toolbox.register('mutate', tools.mutUniformInt, low= 0, up= custom_model.num_layers-1, indpb= 1/num_pruned_layers)\n    \n    # Create the statistics object\n    stats = tools.Statistics(lambda chromosome: chromosome.fitness.values)\n    \n    # Register the statistics object\n    stats.register('max_score', np.max)\n    stats.register('avg_score', np.mean)\n    \n    # Create the hall of fame object\n    hof = tools.HallOfFame(hall_of_fame)\n    \n    # To clear Memory if there is residual in it.\n    clear_cache()\n    \n    # Define the algorithm\n    population, logbook = algorithms.eaSimple(\n        toolbox.PopulationCreator(n= pop_size),\n        toolbox,\n        cxpb= p_cx,\n        mutpb= p_mu,\n        ngen= max_gens,\n        stats= stats,\n        halloffame= hof,\n        verbose= True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:48.130359Z","iopub.execute_input":"2025-02-04T07:42:48.130652Z","iopub.status.idle":"2025-02-04T07:42:48.144070Z","shell.execute_reply.started":"2025-02-04T07:42:48.130615Z","shell.execute_reply":"2025-02-04T07:42:48.143239Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"ready_tasks = downloading_tasks(tasks= ['hellaswag', 'mutual', 'piqa', 'truthfulqa_mc2', 'gsm8k', 'lambada_openai', 'lambada_standard', 'winogrande', 'openbookqa'])\nmetrics = ['acc_norm,none', 'mrr,none', 'acc_norm,none', 'acc,none', 'exact_match,strict-match', 'acc,none', 'acc,none', 'acc,none', 'acc_norm,none']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:42:48.144879Z","iopub.execute_input":"2025-02-04T07:42:48.145083Z","iopub.status.idle":"2025-02-04T07:44:01.432142Z","shell.execute_reply.started":"2025-02-04T07:42:48.145066Z","shell.execute_reply":"2025-02-04T07:44:01.431421Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635ab20a802b41d5b3b0b64f87bc7ff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hellaswag.py:   0%|          | 0.00/4.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f69a653324408ba5455d9a14b88f23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/2.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82eeeba8deeb4e9f99818faa341b2731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e8c502926ec4ec1ae4c015d26834f2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d6cd8cf9744342af4092523c76377d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d990c5ee7b7c4dcdb11b0f33b902faf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5be76977190541a8ac567817ab411da8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a09c4be16fc46e3a270c5cddd6b38dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a45d19470e4880975441fda4059c67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39905 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d370eddfebc643dead9dd3008a250cd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2366f46a355450ea6673eeea7d1f20c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/22.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f697148bb35471481c0b7b943963d15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mutual.py:   0%|          | 0.00/4.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f47b368fa74ad1a75f184566e11012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"862fb5eb7de44b84a54c5483b8e067d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f4d044630f45709172ee642472234e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0190681620df4c84b4dfd6becaa1d6a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"646e9c704b234e3498269de850f63f33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7088 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00bf49049e04201aa1df831b642def4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/886 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac7ea6288584465b464d73cd134ee79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfec9a21f7294574b249fdd147e2dd76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"piqa.py:   0%|          | 0.00/5.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0248c050ed462da099142959675046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf2ae3b88b314337ac7e8121818b325d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/815k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2714718b3e3414188f463371ee3054a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d99cdfd2e9f43dfa73bc9ee8969fd51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3084 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba8882212434642851fa6ad2644fa4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1838 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c191b04574a64099ba6ad81f616b59c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8122a92037bc475783e83bb6f176274a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/271k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a383cec39f6e43c9ae3a57074d6cbbd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dce09d06e524e7588bbb92981d57aba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66cc37acb2f4336b722068e34c2673b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"771621955e1d4a24a6ee25def56274de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972bdce89997404097b27805eddb1273"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b318add3f046e49f08943cc24603d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f523dd848eb48528468b891abdd9fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.99k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5d53286f6a4a3280271538ba4aeb06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"lambada_openai.py:   0%|          | 0.00/4.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c334a5c6c774848ba637cf99b660503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a26cdc82a7942628e69fc619c95a86f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39b3d11edcf40208ee02e4ac7ef6bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f0603aee634f0ba124e0f0cf10c47a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be6f68742b5a4149b2ed132e4bfd8129"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e09947ac4cc421fa8d10ca41fc7a633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33627ff22cf74da0a8138caf6250812d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54454cc53ea740e6947e497b8d691b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2662 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"641a782aa6d74721a1fd8b4adf3d964c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e5e52e22024243802ac63565913439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/4869 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c604c2ee36a14c39a8a9ea6c4c431656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67a649343f0a4e22bbb924a427af0275"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"winogrande.py:   0%|          | 0.00/5.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab766de6567438ba6764f4d197a9f3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6d620f2c6243be99623acf4c179d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/40398 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfea1b1f8fbe4348a839885fe68a7fe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1767 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80b4437a6a904e71a094b67a9ef7153e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1267 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56078e5ddc2147ff9bd45873e8323fb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec9e691e38847058601952d04025031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/496k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d73fad62ee4b8d9c20eedc1692f965"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/58.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ce9f644ac9432c8a6039fa93283acc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/55.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00a313703c6d424bad8c7d5e729e1531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4957 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7a61e25c1e4a768daa7fc793808c80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ca553527776486a9d80198175362f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3e01b2d12846699a77476550d25bb0"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Specify the model name or path\nbase_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:44:01.432936Z","iopub.execute_input":"2025-02-04T07:44:01.433213Z","iopub.status.idle":"2025-02-04T07:44:01.436418Z","shell.execute_reply.started":"2025-02-04T07:44:01.433184Z","shell.execute_reply":"2025-02-04T07:44:01.435811Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Load the configuration\nconfig = AutoConfig.from_pretrained(base_model)\n\ndtype = getattr(config, \"torch_dtype\", None)\ndtype = str(dtype)[6:]\nprint(f\"Data type (dtype): {dtype}\")\n\n# Get the number of layers\nif hasattr(config, \"num_hidden_layers\"):  # Common for transformer-based models\n    num_layers = config.num_hidden_layers\n    print(f\"Number of layers: {num_layers}\")\nelse:\n    print(\"Number of layers not specified in the config.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:44:01.437060Z","iopub.execute_input":"2025-02-04T07:44:01.437241Z","iopub.status.idle":"2025-02-04T07:44:05.102163Z","shell.execute_reply.started":"2025-02-04T07:44:01.437225Z","shell.execute_reply":"2025-02-04T07:44:05.101303Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df2460f098443af96cdaaa4b9467722"}},"metadata":{}},{"name":"stdout","text":"Data type (dtype): bfloat16\nNumber of layers: 28\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"evaluate_base = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:44:05.103020Z","iopub.execute_input":"2025-02-04T07:44:05.103303Z","iopub.status.idle":"2025-02-04T07:44:05.106572Z","shell.execute_reply.started":"2025-02-04T07:44:05.103280Z","shell.execute_reply":"2025-02-04T07:44:05.105835Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"clear_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:44:05.107584Z","iopub.execute_input":"2025-02-04T07:44:05.107865Z","iopub.status.idle":"2025-02-04T07:44:06.032867Z","shell.execute_reply.started":"2025-02-04T07:44:05.107842Z","shell.execute_reply":"2025-02-04T07:44:06.032126Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Evaluate the Base Model\nif evaluate_base:\n    clear_cache()\n    \n    # Start Timer\n    start = time.time()\n    \n    BASE_MODEL_SCORES[base_model] = {}\n    BASE_MODEL_SCORES[base_model]['batch_size'] = 1\n    \n    results = simple_evaluate(\n                        model=\"hf\",\n                        model_args= {'pretrained': base_model,\n                                     'trust_remote_code': True},\n                        tasks = ['hellaswag', 'mutual', 'piqa', 'truthfulqa_mc2', 'gsm8k', 'lambada_openai', 'lambada_standard', 'winogrande', 'openbookqa'],\n                        log_samples= True,\n                        device= \"cuda:0\",\n                        batch_size = 'auto:4',\n                        limit = 100,\n                        task_dict = ready_tasks,\n                        apply_chat_template= False,\n                        )\n\n    # Stop Timer and save the result\n    model_time = time.time() - start\n    BASE_MODEL_SCORES[base_model]['time'] = model_time\n    print(\"Base Model:\", base_model, \"\\nTime Score:\", model_time, \"Seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:47:59.233641Z","iopub.execute_input":"2025-02-04T07:47:59.233983Z","iopub.status.idle":"2025-02-04T08:19:29.455306Z","shell.execute_reply.started":"2025-02-04T07:47:59.233961Z","shell.execute_reply":"2025-02-04T08:19:29.454119Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a496ae1bfd400db25c9ed766f59d60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ad6879cc5af4a16aa44415145ed1856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0512c3c3e0fb4e1b820b552c59782a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ffbf78bf7fe43a29477246e0caa6179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000002.safetensors:   0%|          | 0.00/8.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb59a39322ea400ab9308ac92f6f843f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-000002.safetensors:   0%|          | 0.00/6.62G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"427745b00e6d40e5a48ed041b393731d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db019346d1f45caa5e5f4aca938f6e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"654963bc31894896b9e61b08437686a6"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 100/100 [00:00<00:00, 1832.45it/s]\n100%|██████████| 100/100 [00:00<00:00, 78589.17it/s]\n100%|██████████| 100/100 [00:00<00:00, 512.25it/s]\n100%|██████████| 100/100 [00:00<00:00, 522.37it/s]\n100%|██████████| 100/100 [00:00<00:00, 220.55it/s]\n100%|██████████| 100/100 [00:00<00:00, 703.47it/s]\n100%|██████████| 100/100 [00:00<00:00, 1020.93it/s]\n100%|██████████| 100/100 [00:00<00:00, 1264.33it/s]\n100%|██████████| 100/100 [00:00<00:00, 2380.37it/s]\nRunning loglikelihood requests:   0%|          | 0/2541 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Passed argument batch_size = auto:4.0. Detecting largest batch size\nDetermined largest batch size: 2\n","output_type":"stream"},{"name":"stderr","text":"Running loglikelihood requests:  24%|██▍       | 611/2541 [05:53<16:37,  1.94it/s] ","output_type":"stream"},{"name":"stdout","text":"Passed argument batch_size = auto:4.0. Detecting largest batch size\nDetermined largest batch size: 4\n","output_type":"stream"},{"name":"stderr","text":"Running loglikelihood requests:  48%|████▊     | 1221/2541 [09:53<04:58,  4.43it/s] ","output_type":"stream"},{"name":"stdout","text":"Passed argument batch_size = auto:4.0. Detecting largest batch size\nDetermined largest batch size: 16\n","output_type":"stream"},{"name":"stderr","text":"Running loglikelihood requests:  72%|███████▏  | 1823/2541 [11:39<00:56, 12.81it/s]","output_type":"stream"},{"name":"stdout","text":"Passed argument batch_size = auto:4.0. Detecting largest batch size\n","output_type":"stream"},{"name":"stderr","text":"Running loglikelihood requests:  72%|███████▏  | 1839/2541 [11:54<00:54, 12.81it/s]","output_type":"stream"},{"name":"stdout","text":"Determined largest batch size: 32\n","output_type":"stream"},{"name":"stderr","text":"Running loglikelihood requests:  98%|█████████▊| 2481/2541 [12:25<00:01, 41.86it/s]","output_type":"stream"},{"name":"stdout","text":"Passed argument batch_size = auto:4.0. Detecting largest batch size\nDetermined largest batch size: 64\n","output_type":"stream"},{"name":"stderr","text":"Running loglikelihood requests: 100%|██████████| 2541/2541 [12:29<00:00,  3.39it/s]\nRunning generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Passed argument batch_size = auto. Detecting largest batch size\nDetermined Largest batch size: 1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nRunning generate_until requests: 100%|██████████| 100/100 [17:41<00:00, 10.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"bootstrapping for stddev: perplexity\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:00<00:00, 153.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"bootstrapping for stddev: perplexity\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:00<00:00, 153.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Base Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \nTime Score: 1889.357815027237 Seconds\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# c_model = custom_model(base_model = base_model,\n#                         dtype= dtype,\n#                         num_layers= num_layers)\n# clear_cache()\n# run_ga(custom_model= c_model,\n#        num_pruned_layers= 2,\n#        tasks = ready_tasks,\n#        metrics = metrics,\n#        pop_size = 60,\n#        max_gens = 10,\n#        tourn_size= 5,\n#        p_cx= 0.9,\n#        p_mu= 0.5,\n#        prev_results= {})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:44:06.040730Z","iopub.status.idle":"2025-02-04T07:44:06.040987Z","shell.execute_reply":"2025-02-04T07:44:06.040883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}